{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYU Introduction to Machine Learning\n",
    "## Assignment 2\n",
    "\n",
    "__<font color='red'>Given date: February 20</font>__\n",
    "\n",
    "__<font color='red'>Due Date: March 15</font>__\n",
    "\n",
    "__Total: 25pts__\n",
    "\n",
    "In this assignment you will implement the main regularization approaches as well as cross validation. You will study how the OLS criterion that we used in regression can be extended to classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1. Understanding Cross validation (5pts)\n",
    "\n",
    "Using the lines below load the dataset 'Assignment2_Ex1_xi' and 'Assignment2_Ex1_ti'. Each of the points in the training set is represented by 5 features $x_{i,1}$, $x_{i,2}, \\ldots x_{i,5}$. Among those features we want to find those which are the most meaningful to the description of the targets $t_i$. You can think of the targets as expressing for example the probability to develop a particular trait or disease and the features as encoding the expressivity of particular genes. In such a framework the objective would thus mean finding the genes that most influence the particular trait. For this we will implement a Best Subset Selection approach with cross validation. Complete the cell below by implementing the following steps\n",
    "\n",
    "__1.__ For each number of weights (d=1 to 5) compute all the subsets (beta_i, beta_j, ...) of size d of weights.\n",
    "\n",
    "__2.__ Split the training set in K=5 bins, for each bin k=1,...5, learn the weights by using the linear_regression function of scikit learn (do not reimplement gradient descent except if you really have too much time). Learn the weights on the remaining K-1 bins then comute the MSE on bin k. \n",
    "\n",
    "__3.__ Find the optimal subset of coefficients by comparing the MSE and plot the MSE as a function of the number k of weights by averaging the errors over the size k subsets. I.e MSE(1) = (1/5)(MSE(beta0) + MSE(beta1) + ...MSE(beta4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE for subsets with: \n",
      "\n",
      "1  features =  6.046389346335924\n",
      "2  features =  4.5454991837666014\n",
      "3  features =  3.0340794696417936\n",
      "4  features =  1.5158684720746083\n",
      "5  features =  2.4497751855407863e-30\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xi = np.load('Assignment2_Ex1_xi.npy')\n",
    "ti = np.load('Assignment2_Ex1_ti.npy')\n",
    "\n",
    "D = 5 # number of coefficients \n",
    "\n",
    "K = 5 # number of bins used for cross validation\n",
    "\n",
    "# Note that K does not have to be equal to D \n",
    "# (this is a choice we make here but we could have taken any value for K)\n",
    "\n",
    "# Step 1: Finding the optimal d\n",
    "\n",
    "#Initialization variables\n",
    "\n",
    "MSE_list, avg_MSE, total_mse = [],[],[]\n",
    "\n",
    "print(\"Average MSE for subsets with: \\n\" )\n",
    "# looping through each feature \n",
    "for d in range(1,D + 1):\n",
    "    sum_MSE = 0\n",
    "    \n",
    "    # 1) select each of the (D choose d) subset of coefficient and learn a \n",
    "    # model and compute the MSE\n",
    "    subset_mse = []\n",
    " \n",
    "    for combo in itertools.combinations(range(D),d):\n",
    "        Xi = xi.T[list(combo)]\n",
    "        Xi = Xi.T\n",
    "    \n",
    "        kf = KFold(n_splits=5)\n",
    "\n",
    "        MSE_list = []\n",
    "        \n",
    "        #k fold splitting the data into test/train\n",
    "        for train_index, test_index in kf.split(Xi):\n",
    "            X_train, X_test = Xi[train_index], Xi[test_index]\n",
    "            y_train, y_test = ti[train_index], ti[test_index]\n",
    "            \n",
    "            # fitting the model\n",
    "            L_reg = LinearRegression().fit(X_train,y_train)\n",
    "            predictions = L_reg.predict(X_test)\n",
    "            \n",
    "            # calculating the MSE for each individual bucket of subset \n",
    "            mse = mean_squared_error(y_test, predictions)\n",
    "            MSE_list.append(mse)\n",
    "            \n",
    "        # taking the final average for each subset from cv\n",
    "        favg_MSE = sum(MSE_list) / len(MSE_list)\n",
    "        subset_mse.append(favg_MSE)\n",
    "        total_mse.append(favg_MSE)\n",
    "\n",
    "    # calculating the average for each combination of subset of data\n",
    "    combination = (np.math.factorial(D)/(np.math.factorial(D-d)*np.math.factorial(d)))\n",
    "    avg_MSE.append(sum(subset_mse)/combination)\n",
    "    \n",
    "    final = sum(subset_mse)/len(subset_mse)\n",
    "    print(d,\" features = \", final)\n",
    "\n",
    "        \n",
    "# Step 2 plotting the evolution of the average prediction error as a function of the number of coefficient\n",
    "\n",
    "plt.scatter(range(1,6),avg_MSE)\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Average prediction error')\n",
    "plt.title('The evolution of the average prediction error as a function of the number of coefficient')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2. Predicting graduate admissions (5pts)\n",
    "\n",
    "In this second question, we want to predict admission to graduate school based on a collection of features [provided by Kaggle](https://www.kaggle.com/mohansacharya/graduate-admissions) \n",
    "including: \n",
    "\n",
    "- GRE and TOEFL Scores\n",
    "- University Rating \n",
    "- Letter of Recommendation Strength \n",
    "- Undergraduate GPA \n",
    "- ...\n",
    "    \n",
    "We want to learn a ridge regression model (use the scikit learn model with the fit and predict functions). \n",
    "\n",
    "- Start by splitting the dataset into a training (about 90%) an a test (remaining 10%) parts using a call to the train_test_split function from the model_selection module. Put the test aside for the rest of the exercise. \n",
    "\n",
    "- Now that you are perfectly comfortable with the idea of cross validation, we will also try to evaluate the optimal lambda in the Ridge regression model. For this, you can use an extension of scikit learn Ridge regression model: sklearn.linear_model.RidgeCV. This extension lets you specify an array of $\\lambda$ values ($\\alpha$ in scikit learn) to try. The best value is then returned through a call to the 'alpha_' attribute of the model (read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) for more details). Train the model (both lambda and beta) on the training subset of item 1.\n",
    "\n",
    "- Finally evaluate the prediction of your model on the 10% test set you kept on the side at the beginning. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of alpha:  1.0\n",
      "\n",
      "The MSE for the predictions with Ridge Regression is:  0.00348068759559814\n",
      "R2 Score for ridge:  0.8625850000257349\n",
      "\n",
      "The MSE for the predictions with Linear Regression is:  0.0034765034319357256\n",
      "R2 score for linear: 0.8627501877461986\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_csv(\"Admission_Predict.csv\")\n",
    "\n",
    "# put your code here \n",
    "data = df.values\n",
    "X,y = data[:,:-1], data[:, -1]\n",
    "\n",
    "# splitting the dataset into a training (about 90%) an a test (remaining 10%) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state= 1)\n",
    "\n",
    "\n",
    "# training the model on the training subset of item 1.\n",
    "clf = RidgeCV(alphas=[0.1,0.3,0.87,0.8705,0.9,0.902,1,1.15,1.5]).fit(X_train,y_train)\n",
    "\n",
    "print(\"The best value of alpha: \",clf.alpha_)\n",
    "print()\n",
    "\n",
    "\n",
    "# evaluating the prediction of your model\n",
    "y_prediction = clf.predict(X_test)\n",
    "ridge_error = mean_squared_error(y_test,y_prediction)\n",
    "\n",
    "print(\"The MSE for the predictions with Ridge Regression is: \",ridge_error)\n",
    "print(\"R2 Score for ridge: \", r2_score(y_test, y_prediction))\n",
    "print()\n",
    "\n",
    "\n",
    "#comparing with lienar regression\n",
    "L= LinearRegression()\n",
    "L.fit(X_train, y_train)\n",
    "y_predict = L.predict(X_test)\n",
    "linear_error = mean_squared_error(y_predict,y_test)\n",
    "\n",
    "print(\"The MSE for the predictions with Linear Regression is: \",linear_error)\n",
    "print(\"R2 score for linear:\", r2_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Iterative Hard and Soft Thresholding (10pts)\n",
    "\n",
    "\n",
    "An alternative to the simple OLS criterion or to the Ridge regression model, LASSO regression minimizes a combination of a data fidelity term and a penalty on the sum of the absolute values of the regression coefficients, i.e.\n",
    "\n",
    "\\begin{align}\n",
    "\\ell(\\boldsymbol \\beta ) = \\frac{1}{N}\\sum_{i=1}^N (t^{(i)} - (\\boldsymbol{\\beta}^T \\boldsymbol x^{(i)}))^2 + \\lambda \\sum_{j=0}^D |\\beta_j|, \\quad (\\text{LASSO})\n",
    "\\end{align}\n",
    "\n",
    "One of the main difficulty with the LASSO lies in the non differentiability of the absolute value which appears in the regularization term. Because of the use of the absolute value, the gradient cannot be computed at 0. Instead of relying on gradient updates, we can instead turn to the constrained formulation\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\min & \\quad \\ell(\\boldsymbol \\beta ) = \\frac{1}{N}\\sum_{i=1}^N (t^{(i)} - (\\boldsymbol{\\beta}^T \\boldsymbol x^{(i)}))^2\\\\\n",
    "\\text{subject to}& \\quad \\sum_{j=0}^D |\\beta_j|\\leq t\n",
    "\\end{align}\n",
    "\n",
    "The drawback with such a formulation is that we now have to solve a constrained problem. A common approach relies on the use of thresholding algorithms and in particular to the class of so-called _iterative shrinkage-thresholding algorithms (ISTA)_. If we write the OLS objective in matrix form as $\\ell(\\boldsymbol \\beta) = \\frac{1}{2N}\\|\\tilde{\\mathbf{X}}\\mathbf{\\beta} - \\mathbf{t}\\|_2^2$, Iterative shrinkage-thresholding algorithms are based on the following update :\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{\\beta} \\leftarrow \\mathcal{T}_{\\lambda \\eta}\\left\\{\\mathbf{\\beta} - 2\\eta \\tilde{\\mathbf{X}}^T\\left(\\tilde{\\mathbf{X}}\\mathbf{\\beta} - \\mathbf{t}\\right) \\right\\}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathcal{T}$ is the thresholding operator \n",
    "\\begin{align}\n",
    "\\mathcal{T}_{\\alpha}(\\mathbf{\\beta})_i = \\left(|\\beta_i| - \\alpha\\right)_+\\text{sign}(\\beta_i)\n",
    "\\end{align}\n",
    "\n",
    "Here $\\left(|\\beta_i| - \\alpha\\right)_+ = \\max\\left\\{|\\beta_i| - \\alpha, 0\\right\\}$ and $\\text{sign}(\\beta_i)$ denotes the sign of the coefficient $\\beta_i$. From the definition above, you can also see that $\\lambda \\eta$ acts as a threshold on the $\\beta_i$. The larger $\\lambda$, the more $\\beta_i$ will be set to $0$. \n",
    "\n",
    "\n",
    "#### Question 3.1 (6pts) Complete the function ISTA below which should return a vector of weights $\\mathbf{\\beta}$, starting from some initial guess $\\beta_{\\text{init}}$ and for a training set stored in the matrix $X$ and vector of targets $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ISTA(beta_init, lamb, eta, X, t):\n",
    "    \n",
    "    # function should apply the Iterative Shrinkage \n",
    "    # Thresholding updates, starting from Beta_init and \n",
    "    # for a set of feature vectors stored in matrix X \n",
    "    # with associated targets stored in t.\n",
    "    \n",
    "    no_iter = 1000\n",
    "    # creating the X tilde matrix\n",
    "    Xtilde = np.hstack((np.ones(X.shape[0]).reshape(-1,1),X))\n",
    "    \n",
    "    #starting with a beta matrix of paramater values \n",
    "    beta = np.ones((6,1))*beta_init\n",
    "\n",
    "    t = t.reshape(-1,1)\n",
    "  \n",
    "    for i in range(no_iter): \n",
    "        temp_XB_t = np.dot(Xtilde, beta)-t\n",
    "        \n",
    "        #updating value of beta in every loop \n",
    "        beta =   beta - 2*eta*np.dot(np.transpose(Xtilde), temp_XB_t)\n",
    "        \n",
    "        for i in range(6):\n",
    "            if (beta[i][0] < 0):\n",
    "                beta[i][0] = max(np.abs(beta[i][0])- eta*lamb,0)*(-1)\n",
    "            else:\n",
    "                beta[i][0] = max(np.abs(beta[i][0])- eta*lamb,0)\n",
    "                \n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2 (4pts) Test your algorithm on the dataset provided by the two files 'Assignment2_Ex32_Xi.npy' and 'Assignment2_Ex32_ti.npy' below. as above, two of the weights are irrelevant. Try to tune the parameters $\\eta$ and $\\lambda$ and study when you can recover those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspect beta_ISTA: \n",
      "\n",
      "0.0\n",
      "1.804001433459846\n",
      "0.6847282299141919\n",
      "0.00010902208961878613\n",
      "0.5061634611785284\n",
      "3.6399733816259156e-05\n"
     ]
    }
   ],
   "source": [
    "X = np.load('Assignment2_Ex32_Xi.npy')\n",
    "t = np.load('Assignment2_Ex32_ti.npy')\n",
    "\n",
    "\n",
    "# put your code here \n",
    "pred = ISTA(5,0.002,0.0001,X,t)\n",
    "\n",
    "print(\"Inspect beta_ISTA: \\n\")\n",
    "for i in range(len(pred)):\n",
    "    print(pred[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (5pts). From regression to binary classification: Predicting deaths on the Titanic\n",
    "\n",
    "We have seen how the OLS objective can be used to learn a regression model. This objective remains in fact absolutely valid in the classification framework. In binary classification, the targets associated to the feature vectors take one of two values (let us say $1$ and $0$ or $+1$ and $-1$). If we want to learn a model that classifies some feature vectors $\\mathbf{x}^{(i)}$ as belonging to class $\\mathcal{C}_0$ vs $\\mathcal{C}_1$ and we are given a training set $C_{0, \\text{tr}} = \\left\\{\\mathbf{x}^{(i)}\\right\\}_{i=1}^{N_0}$ and $C_{1, \\text{tr}} = \\left\\{\\mathbf{x}^{(j)}\\right\\}_{j=1}^{N_1}$, we can try to learn a separating plane $\\beta_0 +\\beta_1 x_1 + \\ldots \\beta_D x_D$ such that $\\beta_0 +\\beta_1 x^{(i)}_1 + \\ldots \\beta_D x^{(i)}_D =+1 $ for all $x^{(i)}\\in C_0$ and $\\beta_0 +\\beta_1 x^{(j)}_1 + \\ldots \\beta_D x^{(j)}_D =-1$ for all $x^{(j)}$ in $\\mathcal{C}_1$. \n",
    "\n",
    "For any new point $\\mathbf{x}$ of unknown class, we can then compute $\\beta_0 +\\beta_1x_1 + \\ldots +\\beta_D x_D$ and classify our point as belonging to $C_0$ if $\\beta_0 +\\beta_1x_1 + \\ldots +\\beta_D x_D>0$.\n",
    "\n",
    "Combine this idea with the linear regression model from scikit learn to learn a linear binary classifier for the ['Titanic'](https://www.kaggle.com/c/titanic/data?select=test.csv) dataset from Kaggle. Start by loading the training and test data from this dataset and then complete the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 78.94736842105263 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Step 1. \n",
    "# =========================================================================\n",
    "# Use the linearRegression model from scikit learn with binary \n",
    "# targets to predict the passengers that will survive and di in the \n",
    "# case of the sinking of a ship. Start by turning the class targets to \n",
    "# binary or +1/-1 values. Then turn possible non numeric features to numbers. Finally \n",
    "# learn the separating plane.\n",
    "\n",
    "# dropping the row with no data \n",
    "training_data = training_data.dropna()\n",
    "\n",
    "training_data.Sex[training_data.Sex == 'male'] = 1\n",
    "training_data.Sex[training_data.Sex == 'female'] = 2\n",
    "\n",
    "training_data.Embarked[training_data.Embarked == 'S'] = 1\n",
    "training_data.Embarked[training_data.Embarked == 'C'] = 2\n",
    "training_data.Embarked[training_data.Embarked == 'Q'] = 3\n",
    "\n",
    "\n",
    "# setting the training set\n",
    "target_data = training_data[\"Survived\"]\n",
    "\n",
    "\n",
    "#dropping the not required columns\n",
    "training_data.drop(['Name', 'Ticket','PassengerId','Cabin','Fare','Survived'],axis='columns', inplace=True)\n",
    "\n",
    "\n",
    "# Splitting in testing and training data because we do not have target in train.csv\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data.values.tolist(),target_data.values.tolist(), test_size=0.1, random_state= 1)\n",
    "\n",
    "\n",
    "# training the model \n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Step 2. \n",
    "# =========================================================================\n",
    "# Validate your model on the test set and compute the fraction of correctly \n",
    "# classified samples using the function accuracy_score from the sklearn.metrics module\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions = lin.predict(X_test)\n",
    "final_predictions = []\n",
    "\n",
    "for i in predictions:\n",
    "    if i<=0.5:\n",
    "        final_predictions.append(0)\n",
    "    else:\n",
    "        final_predictions.append(1)\n",
    "        \n",
    "acc_s = accuracy_score(y_test, final_predictions)\n",
    "\n",
    "print(\"Accuracy\",acc_s*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
